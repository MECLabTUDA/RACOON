{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Precision, Recall (Sensitivity, Specificity) and Error calculation for JIP models </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook calculates the Precision, Recall (Sensitivity, Specificity) and Error of the predictions made by the JIP models for the test datasets.\n",
    "Before executing this Notebook, be sure to have trained all 6 artefact models using the provided code in three steps:\n",
    "    \n",
    "1. Preprocess all datasets (train and test) using the following command:\n",
    "```bash\n",
    "python JIP.py --mode preprocess --device <cuda_id> --datatype train\n",
    "```  \n",
    "and   \n",
    "```bash\n",
    "python JIP.py --mode preprocess --device <cuda_id> --datatype test\n",
    "```\n",
    "2. Train all 6 models using the following command:\n",
    "```bash\n",
    "python JIP.py --mode train --device <cuda_id> --datatype train \n",
    "                 --noise_type <noise_model> --store_data\n",
    "```\n",
    "3. Perform the testing as follows:\n",
    "```bash\n",
    "python JIP.py --mode testIOOD --device <cuda_id> --datatype test\n",
    "                 --noise_type <noise_model> --store_data\n",
    "```\n",
    "\n",
    "\n",
    "Once this is finished, everything is set up to run the Notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "from itertools import zip_longest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# -- Grouper from https://stackoverflow.com/questions/8991506/iterate-an-iterator-by-chunks-of-n-in-python -- #\n",
    "def grouper(n, iterable, fillvalue=None):\n",
    "    \"grouper(3, 'ABCDEFG', 'x') --> ABC DEF Gxx\"\n",
    "    args = [iter(iterable)] * n\n",
    "    return zip_longest(fillvalue=fillvalue, *args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set necessary directories\n",
    "Specify the train_base and test_base directory. These are just the full paths to the JIP folder train_dirs and test_dirs output, for instance: `../JIP/train_dirs/output` and `../JIP/test_dirs/output`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the base path to JIP/train_dirs/output folder\n",
    "train_base = '<path>/JIP/train_dirs/output/'\n",
    "# Set the base path to JIP/test_dirs/output folder\n",
    "test_base = '<path>/JIP/test_dirs/output/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "artefacts = ['blur', 'ghosting', 'motion', 'noise', 'resolution', 'spike']\n",
    "\n",
    "data = dict()\n",
    "for artefact in artefacts:\n",
    "    # Load data\n",
    "    dl = np.load(os.path.join(train_base, artefact, 'results', 'accuracy_detailed_test.npy'))\n",
    "    ID = np.load(os.path.join(test_base, artefact, 'testID_results', 'accuracy_detailed_test.npy'))\n",
    "    OOD = np.load(os.path.join(test_base, artefact, 'testOOD_results', 'accuracy_detailed_test.npy'))\n",
    "    \n",
    "    # Create One Hot vectors from predicted values\n",
    "    for idx, a in enumerate(dl):\n",
    "        b = np.zeros_like(a[1])\n",
    "        b[a[1].argmax()] = 1\n",
    "        a[1] = b\n",
    "        dl[idx] = a\n",
    "    for idx, a in enumerate(ID):\n",
    "        b = np.zeros_like(a[1])\n",
    "        b[a[1].argmax()] = 1\n",
    "        a[1] = b\n",
    "        ID[idx] = a\n",
    "    for idx, a in enumerate(OOD):\n",
    "        b = np.zeros_like(a[1])\n",
    "        b[a[1].argmax()] = 1\n",
    "        a[1] = b\n",
    "        OOD[idx] = a\n",
    "        \n",
    "    # Save data in dictionary\n",
    "    data['test_dl-' + artefact] = dl\n",
    "    data['test_ID-' + artefact] = ID\n",
    "    data['test_OOD-' + artefact] = OOD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform data into Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data into right format for calculations\n",
    "y_yhats = dict()\n",
    "# Loop through all data sets\n",
    "for k, v in data.items():\n",
    "    y_yhats[k] = dict()\n",
    "    y_yhats[k]['prediction'] = list()\n",
    "    y_yhats[k]['ground_truth'] = list()\n",
    "    # Change the format of y_yhats --> split in prediction and GT\n",
    "    for y_yhat in v:\n",
    "        y_yhats[k]['prediction'].append(y_yhat[1])\n",
    "        y_yhats[k]['ground_truth'].append(y_yhat[0])\n",
    "    y_yhats[k]['prediction'] = np.array(y_yhats[k]['prediction'])\n",
    "    y_yhats[k]['ground_truth'] = np.array(y_yhats[k]['ground_truth'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate a Classification Report including Precision, Recall and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: In every confusion matrix: x-axis --> predicted, y-axis --> actual.\n",
      "\n",
      "test_dl-blur:\n",
      "Confusion Matrix\n",
      "\n",
      "[[195  53   2  33   5]\n",
      " [  8 176  21  11   7]\n",
      " [ 39  33 157  14  15]\n",
      " [  1  16  32  59  19]\n",
      " [ 18  22  53  21 190]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.68      0.75      0.71       261\n",
      "   Quality 2       0.79      0.59      0.67       300\n",
      "   Quality 3       0.61      0.59      0.60       265\n",
      "   Quality 4       0.46      0.43      0.45       138\n",
      "   Quality 5       0.62      0.81      0.70       236\n",
      "\n",
      "   micro avg       0.65      0.65      0.65      1200\n",
      "   macro avg       0.63      0.63      0.63      1200\n",
      "weighted avg       0.66      0.65      0.64      1200\n",
      " samples avg       0.65      0.65      0.65      1200\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.6328830124823145\n",
      "Mean recall (without avg values): 0.6317733822567451\n",
      "Mean f1-score (without avg values): 0.6265583596748644\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.6475\n",
      "micro avg -- specificity: 0.911875\n",
      "\n",
      "test_ID-blur:\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0 23  9 35 13]\n",
      " [ 0  6  5 18 19]\n",
      " [ 0  0  2  0 30]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         0\n",
      "   Quality 2       0.00      0.00      0.00        29\n",
      "   Quality 3       0.11      0.56      0.19        16\n",
      "   Quality 4       0.38      0.34      0.36        53\n",
      "   Quality 5       0.94      0.48      0.64        62\n",
      "\n",
      "   micro avg       0.36      0.36      0.36       160\n",
      "   macro avg       0.29      0.28      0.24       160\n",
      "weighted avg       0.50      0.36      0.38       160\n",
      " samples avg       0.36      0.36      0.36       160\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.35625\n",
      "Mean recall (without avg values): 0.34649840231284235\n",
      "Mean f1-score (without avg values): 0.29555837897619547\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.35625\n",
      "micro avg -- specificity: 0.8390625\n",
      "\n",
      "test_OOD-blur:\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0 12  2 40 42]\n",
      " [ 0 15  9 92 12]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         0\n",
      "   Quality 2       0.00      0.00      0.00        27\n",
      "   Quality 3       0.00      0.00      0.00        11\n",
      "   Quality 4       0.42      0.30      0.35       132\n",
      "   Quality 5       0.09      0.22      0.13        54\n",
      "\n",
      "   micro avg       0.23      0.23      0.23       224\n",
      "   macro avg       0.10      0.11      0.10       224\n",
      "weighted avg       0.27      0.23      0.24       224\n",
      " samples avg       0.23      0.23      0.23       224\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.12760416666666669\n",
      "Mean recall (without avg values): 0.13131313131313133\n",
      "Mean f1-score (without avg values): 0.12068633121264702\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.23214285714285715\n",
      "micro avg -- specificity: 0.8080357142857143\n",
      "\n",
      "test_dl-ghosting:\n",
      "Confusion Matrix\n",
      "\n",
      "[[192  62   0   0   3]\n",
      " [ 62 175   0   0   3]\n",
      " [  0   0 171  21   1]\n",
      " [  0   0  71 239   8]\n",
      " [ 46  18  24  23  81]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.75      0.64      0.69       300\n",
      "   Quality 2       0.73      0.69      0.71       255\n",
      "   Quality 3       0.89      0.64      0.75       266\n",
      "   Quality 4       0.75      0.84      0.80       283\n",
      "   Quality 5       0.42      0.84      0.56        96\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1200\n",
      "   macro avg       0.71      0.73      0.70      1200\n",
      "weighted avg       0.75      0.71      0.72      1200\n",
      " samples avg       0.71      0.71      0.71      1200\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.7071412136934498\n",
      "Mean recall (without avg values): 0.7314809241717889\n",
      "Mean f1-score (without avg values): 0.6998834769702167\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.715\n",
      "micro avg -- specificity: 0.92875\n",
      "\n",
      "test_ID-ghosting:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   8   0   1 151]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         0\n",
      "   Quality 2       0.00      0.00      0.00         8\n",
      "   Quality 3       0.00      0.00      0.00         0\n",
      "   Quality 4       0.00      0.00      0.00         1\n",
      "   Quality 5       0.94      1.00      0.97       151\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       160\n",
      "   macro avg       0.19      0.20      0.19       160\n",
      "weighted avg       0.89      0.94      0.92       160\n",
      " samples avg       0.94      0.94      0.94       160\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.3145833333333333\n",
      "Mean recall (without avg values): 0.3333333333333333\n",
      "Mean f1-score (without avg values): 0.32368703108252944\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.94375\n",
      "micro avg -- specificity: 0.9859375\n",
      "\n",
      "test_OOD-ghosting:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  1   6   0   6 211]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         1\n",
      "   Quality 2       0.00      0.00      0.00         6\n",
      "   Quality 3       0.00      0.00      0.00         0\n",
      "   Quality 4       0.00      0.00      0.00         6\n",
      "   Quality 5       0.94      1.00      0.97       211\n",
      "\n",
      "   micro avg       0.94      0.94      0.94       224\n",
      "   macro avg       0.19      0.20      0.19       224\n",
      "weighted avg       0.89      0.94      0.91       224\n",
      " samples avg       0.94      0.94      0.94       224\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.23549107142857142\n",
      "Mean recall (without avg values): 0.25\n",
      "Mean f1-score (without avg values): 0.2425287356321839\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.9419642857142857\n",
      "micro avg -- specificity: 0.9854910714285714\n",
      "\n",
      "test_dl-motion:\n",
      "Confusion Matrix\n",
      "\n",
      "[[137  74   9   6   0]\n",
      " [ 86 106  11   4   0]\n",
      " [ 44  32  95  58  10]\n",
      " [ 20  15  16 138   4]\n",
      " [ 33  23  28 109 142]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.61      0.43      0.50       320\n",
      "   Quality 2       0.51      0.42      0.46       250\n",
      "   Quality 3       0.40      0.60      0.48       159\n",
      "   Quality 4       0.72      0.44      0.54       315\n",
      "   Quality 5       0.42      0.91      0.58       156\n",
      "\n",
      "   micro avg       0.52      0.52      0.52      1200\n",
      "   macro avg       0.53      0.56      0.51      1200\n",
      "weighted avg       0.56      0.52      0.51      1200\n",
      " samples avg       0.52      0.52      0.52      1200\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.5309336056902216\n",
      "Mean recall (without avg values): 0.5595921850162416\n",
      "Mean f1-score (without avg values): 0.5129663791183201\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.515\n",
      "micro avg -- specificity: 0.87875\n",
      "\n",
      "test_ID-motion:\n",
      "Confusion Matrix\n",
      "\n",
      "[[ 0  0  0  0  0]\n",
      " [ 0  0  0  0  0]\n",
      " [ 0  0  2  2 12]\n",
      " [ 0  0  2 14 64]\n",
      " [ 0  0  1  9 54]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         0\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.12      0.40      0.19         5\n",
      "   Quality 4       0.17      0.56      0.27        25\n",
      "   Quality 5       0.84      0.42      0.56       130\n",
      "\n",
      "   micro avg       0.44      0.44      0.44       160\n",
      "   macro avg       0.23      0.28      0.20       160\n",
      "weighted avg       0.72      0.44      0.50       160\n",
      " samples avg       0.44      0.44      0.44       160\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.38125000000000003\n",
      "Mean recall (without avg values): 0.4584615384615385\n",
      "Mean f1-score (without avg values): 0.3379479626902307\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.4375\n",
      "micro avg -- specificity: 0.859375\n",
      "\n",
      "test_OOD-motion:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   6  50 168]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         0\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         6\n",
      "   Quality 4       0.00      0.00      0.00        50\n",
      "   Quality 5       0.75      1.00      0.86       168\n",
      "\n",
      "   micro avg       0.75      0.75      0.75       224\n",
      "   macro avg       0.15      0.20      0.17       224\n",
      "weighted avg       0.56      0.75      0.64       224\n",
      " samples avg       0.75      0.75      0.75       224\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.25\n",
      "Mean recall (without avg values): 0.3333333333333333\n",
      "Mean f1-score (without avg values): 0.2857142857142857\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.75\n",
      "micro avg -- specificity: 0.9375\n",
      "\n",
      "test_dl-noise:\n",
      "Confusion Matrix\n",
      "\n",
      "[[175  36  20   8   1]\n",
      " [122  67  49  14   5]\n",
      " [ 57  19  88  45  30]\n",
      " [ 46  20  36 101  53]\n",
      " [ 30  20  20  22 116]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.73      0.41      0.52       430\n",
      "   Quality 2       0.26      0.41      0.32       162\n",
      "   Quality 3       0.37      0.41      0.39       213\n",
      "   Quality 4       0.39      0.53      0.45       190\n",
      "   Quality 5       0.56      0.57      0.56       205\n",
      "\n",
      "   micro avg       0.46      0.46      0.46      1200\n",
      "   macro avg       0.46      0.47      0.45      1200\n",
      "weighted avg       0.52      0.46      0.47      1200\n",
      " samples avg       0.46      0.46      0.46      1200\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.4620582900568233\n",
      "Mean recall (without avg values): 0.46622702738214733\n",
      "Mean f1-score (without avg values): 0.44924715989959807\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.4558333333333333\n",
      "micro avg -- specificity: 0.8639583333333334\n",
      "\n",
      "test_ID-noise:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  2   0   1   5  24]\n",
      " [  0   0   1  14 113]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         2\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         2\n",
      "   Quality 4       0.16      0.26      0.20        19\n",
      "   Quality 5       0.88      0.82      0.85       137\n",
      "\n",
      "   micro avg       0.74      0.74      0.74       160\n",
      "   macro avg       0.21      0.22      0.21       160\n",
      "weighted avg       0.77      0.74      0.75       160\n",
      " samples avg       0.74      0.74      0.74       160\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.259765625\n",
      "Mean recall (without avg values): 0.2719938532462543\n",
      "Mean f1-score (without avg values): 0.2622271550129486\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.7375\n",
      "micro avg -- specificity: 0.934375\n",
      "\n",
      "test_OOD-noise:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  2   0   1   3  42]\n",
      " [  0   0   0  17 159]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         2\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         1\n",
      "   Quality 4       0.06      0.15      0.09        20\n",
      "   Quality 5       0.90      0.79      0.84       201\n",
      "\n",
      "   micro avg       0.72      0.72      0.72       224\n",
      "   macro avg       0.19      0.19      0.19       224\n",
      "weighted avg       0.82      0.72      0.76       224\n",
      " samples avg       0.72      0.72      0.72       224\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.24147727272727273\n",
      "Mean recall (without avg values): 0.23526119402985074\n",
      "Mean f1-score (without avg values): 0.2329341550943985\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.7232142857142857\n",
      "micro avg -- specificity: 0.9308035714285714\n",
      "\n",
      "test_dl-resolution:\n",
      "Confusion Matrix\n",
      "\n",
      "[[181   3   5   5  30]\n",
      " [  2 220  80   4   1]\n",
      " [  8  12 153  84  13]\n",
      " [  0   0   5 166   6]\n",
      " [ 44  29   1  17 131]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.81      0.77      0.79       235\n",
      "   Quality 2       0.72      0.83      0.77       264\n",
      "   Quality 3       0.57      0.63      0.60       244\n",
      "   Quality 4       0.94      0.60      0.73       276\n",
      "   Quality 5       0.59      0.72      0.65       181\n",
      "\n",
      "   micro avg       0.71      0.71      0.71      1200\n",
      "   macro avg       0.72      0.71      0.71      1200\n",
      "weighted avg       0.74      0.71      0.71      1200\n",
      " samples avg       0.71      0.71      0.71      1200\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.7238515912474534\n",
      "Mean recall (without avg values): 0.7111602922116632\n",
      "Mean f1-score (without avg values): 0.7075191196846656\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.7091666666666666\n",
      "micro avg -- specificity: 0.9272916666666666\n",
      "\n",
      "test_ID-resolution:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   1  15]\n",
      " [  0   0   0   0   0]\n",
      " [  3   0   0   2 139]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         3\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         0\n",
      "   Quality 4       0.00      0.00      0.00         3\n",
      "   Quality 5       0.97      0.90      0.93       154\n",
      "\n",
      "   micro avg       0.87      0.87      0.87       160\n",
      "   macro avg       0.19      0.18      0.19       160\n",
      "weighted avg       0.93      0.87      0.90       160\n",
      " samples avg       0.87      0.87      0.87       160\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.32175925925925924\n",
      "Mean recall (without avg values): 0.3008658008658009\n",
      "Mean f1-score (without avg values): 0.3109619686800895\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.86875\n",
      "micro avg -- specificity: 0.9671875\n",
      "\n",
      "test_OOD-resolution:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  5   0   0  29 190]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         5\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         0\n",
      "   Quality 4       0.00      0.00      0.00        29\n",
      "   Quality 5       0.85      1.00      0.92       190\n",
      "\n",
      "   micro avg       0.85      0.85      0.85       224\n",
      "   macro avg       0.17      0.20      0.18       224\n",
      "weighted avg       0.72      0.85      0.78       224\n",
      " samples avg       0.85      0.85      0.85       224\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.28273809523809523\n",
      "Mean recall (without avg values): 0.3333333333333333\n",
      "Mean f1-score (without avg values): 0.3059581320450886\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.8482142857142857\n",
      "micro avg -- specificity: 0.9620535714285714\n",
      "\n",
      "test_dl-spike:\n",
      "Confusion Matrix\n",
      "\n",
      "[[193  16   8   5   4]\n",
      " [  0 181  25  30   4]\n",
      " [  0 148  98  51   5]\n",
      " [  0  63  12 115  49]\n",
      " [ 14  29   6  19 125]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.85      0.93      0.89       207\n",
      "   Quality 2       0.75      0.41      0.53       437\n",
      "   Quality 3       0.32      0.66      0.43       149\n",
      "   Quality 4       0.48      0.52      0.50       220\n",
      "   Quality 5       0.65      0.67      0.66       187\n",
      "\n",
      "   micro avg       0.59      0.59      0.59      1200\n",
      "   macro avg       0.61      0.64      0.60      1200\n",
      "weighted avg       0.65      0.59      0.60      1200\n",
      " samples avg       0.59      0.59      0.59      1200\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.6122984441418878\n",
      "Mean recall (without avg values): 0.6390898768345307\n",
      "Mean f1-score (without avg values): 0.6039481583623503\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.5933333333333334\n",
      "micro avg -- specificity: 0.8983333333333333\n",
      "\n",
      "test_ID-spike:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [ 13   0   1   5 141]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00        13\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         1\n",
      "   Quality 4       0.00      0.00      0.00         5\n",
      "   Quality 5       0.88      1.00      0.94       141\n",
      "\n",
      "   micro avg       0.88      0.88      0.88       160\n",
      "   macro avg       0.18      0.20      0.19       160\n",
      "weighted avg       0.78      0.88      0.83       160\n",
      " samples avg       0.88      0.88      0.88       160\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.2203125\n",
      "Mean recall (without avg values): 0.25\n",
      "Mean f1-score (without avg values): 0.23421926910299\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.88125\n",
      "micro avg -- specificity: 0.9703125\n",
      "\n",
      "test_OOD-spike:\n",
      "Confusion Matrix\n",
      "\n",
      "[[  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  0   0   0   0   0]\n",
      " [  3   0   0   2  11]\n",
      " [  6   0   1  22 179]]\n",
      "\n",
      "Classification Report\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   Quality 1       0.00      0.00      0.00         9\n",
      "   Quality 2       0.00      0.00      0.00         0\n",
      "   Quality 3       0.00      0.00      0.00         1\n",
      "   Quality 4       0.12      0.08      0.10        24\n",
      "   Quality 5       0.86      0.94      0.90       190\n",
      "\n",
      "   micro avg       0.81      0.81      0.81       224\n",
      "   macro avg       0.20      0.21      0.20       224\n",
      "weighted avg       0.74      0.81      0.77       224\n",
      " samples avg       0.81      0.81      0.81       224\n",
      "\n",
      "\n",
      "Summarized Report\n",
      "\n",
      "Mean precision (without avg values): 0.24639423076923078\n",
      "Mean recall (without avg values): 0.256359649122807\n",
      "Mean f1-score (without avg values): 0.24987437185929648\n",
      "--------------------------------------------------------------\n",
      "micro avg -- sensitivity: 0.8080357142857143\n",
      "micro avg -- specificity: 0.9520089285714286\n"
     ]
    }
   ],
   "source": [
    "print('NOTE: In every confusion matrix: x-axis --> predicted, y-axis --> actual.')\n",
    "# Loop through the transformed data and calculate everything\n",
    "for test_name, results in y_yhats.items():\n",
    "    confusion = confusion_matrix(results['ground_truth'].argmax(axis=1),\n",
    "                                 results['prediction'].argmax(axis=1),\n",
    "                                 labels=[0, 1, 2, 3, 4])\n",
    "    print('\\n{}:'.format(test_name))\n",
    "    print('Confusion Matrix\\n')\n",
    "    print(confusion)\n",
    "    \n",
    "    print('\\nClassification Report\\n')\n",
    "    print(classification_report(results['prediction'], results['ground_truth'],\n",
    "                                target_names=['Quality 1', 'Quality 2', 'Quality 3', 'Quality 4', 'Quality 5']))\n",
    "    \n",
    "    # Calculate mean values for each element in the loop\n",
    "    print('\\nSummarized Report\\n')\n",
    "    report = classification_report(results['prediction'], results['ground_truth'],\n",
    "                        target_names=['Quality 1', 'Quality 2', 'Quality 3', 'Quality 4', 'Quality 5'],\n",
    "                        output_dict=True)\n",
    "    \n",
    "    precision, recall, f1 = list(), list(), list()\n",
    "    idx = 0\n",
    "    for k, v in report.items():\n",
    "        if v['support'] != 0 and idx < 5:\n",
    "            precision.append(v['precision'])\n",
    "            recall.append(v['recall'])\n",
    "            f1.append(v['f1-score'])\n",
    "        idx += 1\n",
    "    print('Mean precision (without avg values): {}'.format(sum(precision)/len(precision)))\n",
    "    print('Mean recall (without avg values): {}'.format(sum(recall)/len(recall)))\n",
    "    print('Mean f1-score (without avg values): {}'.format(sum(f1)/len(f1)))\n",
    "    print('--------------------------------------------------------------')\n",
    "    \n",
    "    FP = confusion.sum(axis=0) - np.diag(confusion)\n",
    "    FN = confusion.sum(axis=1) - np.diag(confusion)\n",
    "    TP = np.diag(confusion)\n",
    "    TN = confusion.sum() - (FP + FN + TP)\n",
    "    FP = FP.astype(float)\n",
    "    FN = FN.astype(float)\n",
    "    TP = TP.astype(float)\n",
    "    TN = TN.astype(float)\n",
    "    \"\"\"\n",
    "    # Sensitivity, hit rate, recall, or true positive rate\n",
    "    TPR = TP/(TP+FN)\n",
    "    # Specificity or true negative rate\n",
    "    TNR = TN/(TN+FP) \n",
    "    # Precision or positive predictive value\n",
    "    PPV = TP/(TP+FP)\n",
    "    # Negative predictive value\n",
    "    NPV = TN/(TN+FN)\n",
    "    # Fall out or false positive rate\n",
    "    FPR = FP/(FP+TN)\n",
    "    # False negative rate\n",
    "    FNR = FN/(TP+FN)\n",
    "    # False discovery rate\n",
    "    FDR = FP/(TP+FP)\n",
    "    # Overall accuracy for each class\n",
    "    ACC = (TP+TN)/(TP+FP+FN+TN)\"\"\"\n",
    "    \n",
    "    # Each variable holds the values for every class, so calculate micro avg\n",
    "    Sensitivity = TP.sum()/ (TP.sum() + FN.sum())\n",
    "    Specificity = TN.sum()/ (TN.sum() + FP.sum())\n",
    "    \n",
    "    print('micro avg -- sensitivity: {}'.format(Sensitivity))\n",
    "    print('micro avg -- specificity: {}'.format(Specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the error based on absolute difference between y and yhat (Mean Absolute Error)\n",
    "\n",
    "First of all, the Mean Absolute Error is calculated, which is simply the absolute error between the ground truth and the predicted quality.\n",
    "\n",
    "Example:\n",
    "\n",
    "* Ground truth for the quality of three images is $$y = [2, 5, 2]$$\n",
    "* Model predictions: $$\\hat y = [1, 3, 5]$$\n",
    "* MAE would be: $$MAE(y, \\hat y) = \\frac{\\sum_{i=0}^N |y_{i} - \\hat y_{i}|}{N} = \\frac{|2 - 1| + |5 - 3| + |2 - 5|}{3} = \\frac{6}{3} = 2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE results for each test:\n",
      "\n",
      "test_dl-blur: 0.63\n",
      "test_ID-blur: 0.78\n",
      "test_OOD-blur: 1.00\n",
      "test_dl-ghosting: 0.46\n",
      "test_ID-ghosting: 0.16\n",
      "test_OOD-ghosting: 0.12\n",
      "test_dl-motion: 0.74\n",
      "test_ID-motion: 0.64\n",
      "test_OOD-motion: 0.28\n",
      "test_dl-noise: 0.89\n",
      "test_ID-noise: 0.29\n",
      "test_OOD-noise: 0.29\n",
      "test_dl-resolution: 0.56\n",
      "test_ID-resolution: 0.28\n",
      "test_OOD-resolution: 0.22\n",
      "test_dl-spike: 0.61\n",
      "test_ID-spike: 0.37\n",
      "test_OOD-spike: 0.30\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE results for each test:\\n\")\n",
    "# Loop through the transformed data and calculate MAE\n",
    "for test_name, results in y_yhats.items():\n",
    "    print('{}: {:.2f}'.format(test_name, mean_absolute_error(results['prediction'].argmax(axis=1),\n",
    "                                                             results['ground_truth'].argmax(axis=1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE results for each model:\n",
      "\n",
      "Model blur: 0.80\n",
      "Model ghosting: 0.25\n",
      "Model motion: 0.55\n",
      "Model noise: 0.49\n",
      "Model resolution: 0.35\n",
      "Model spike: 0.43\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE results for each model:\\n\")\n",
    "# Loop through the transformed data and calculate MAE\n",
    "for (t1, results1), (t2, results2), (t3, results3) in grouper(3, y_yhats.items()):\n",
    "    model = t1.split('-')[1]\n",
    "    avg_mae = mean_absolute_error(results1['prediction'].argmax(axis=1),\n",
    "                                  results1['ground_truth'].argmax(axis=1))\n",
    "    avg_mae += mean_absolute_error(results2['prediction'].argmax(axis=1),\n",
    "                                   results2['ground_truth'].argmax(axis=1))\n",
    "    avg_mae += mean_absolute_error(results3['prediction'].argmax(axis=1),\n",
    "                                   results3['ground_truth'].argmax(axis=1))\n",
    "    \n",
    "    print('Model {}: {:.2f}'.format(model, avg_mae/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a next step, we want an error metric, that shows how far off the model predicts on average. For this, we use the MAE and divide it by the possible number of (quality) classes - 1, here $5 - 1 = 4$. For instance this means that a higher error of two model indicates the predictions of the one model with higher error are less good than the one with the smaller error. So the higher the error, the far off the predictions are from the ground truth values. Maximum error is 100% and would be the case if all images have a quality of 5 and the model predicts a quality of 1 for every image, assuming that the possible values are $(1, 2, 3, 4, 5)$, ie. 5 in total. However the greatest distance a model can achieve is 4 (predict 1 and actual is 5) --> very bad. Let's have a look at a small example:\n",
    "\n",
    "* Ground truth for the quality of three images is $$y = [2, 5, 2]$$\n",
    "* Model one makes the following predictions: $$\\hat y^{1} = [1, 3, 5]$$\n",
    "* Model two makes the following predictions: $$\\hat y^{2} = [2, 4, 3]$$\n",
    "\n",
    "Obviously one might correctly assume that model two makes better predictions than model one. Likewise, the error for model two is lower than model one:\n",
    "\n",
    "* Error for model one: $$\\frac{MAE(y, \\hat y^{1})}{4} = \\frac{2}{4} \\Rightarrow 50\\%$$ \n",
    "* Error for model two: $$\\frac{MAE(y, \\hat y^{2})}{4} = \\frac{0.667}{4} \\Rightarrow 16. 67\\%$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error results for each test:\n",
      "\n",
      "test_dl-blur: 15.71%\n",
      "test_ID-blur: 19.38%\n",
      "test_OOD-blur: 24.89%\n",
      "test_dl-ghosting: 11.58%\n",
      "test_ID-ghosting: 3.91%\n",
      "test_OOD-ghosting: 3.12%\n",
      "test_dl-motion: 18.52%\n",
      "test_ID-motion: 16.09%\n",
      "test_OOD-motion: 6.92%\n",
      "test_dl-noise: 22.19%\n",
      "test_ID-noise: 7.34%\n",
      "test_OOD-noise: 7.37%\n",
      "test_dl-resolution: 14.00%\n",
      "test_ID-resolution: 7.03%\n",
      "test_OOD-resolution: 5.47%\n",
      "test_dl-spike: 15.21%\n",
      "test_ID-spike: 9.22%\n",
      "test_OOD-spike: 7.59%\n"
     ]
    }
   ],
   "source": [
    "print(\"Error results for each test:\\n\")\n",
    "\n",
    "# Loop through the transformed data and calculate the error by dividing by number_classes\n",
    "for test_name, results in y_yhats.items():\n",
    "    print('{}: {:.2f}%'.format(test_name, 100*(mean_absolute_error(results['prediction'].argmax(axis=1),\n",
    "                                                                   results['ground_truth'].argmax(axis=1))/4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average error results for each model:\n",
      "\n",
      "Model blur: 19.99%\n",
      "Model ghosting: 6.20%\n",
      "Model motion: 13.84%\n",
      "Model noise: 12.30%\n",
      "Model resolution: 8.83%\n",
      "Model spike: 10.67%\n"
     ]
    }
   ],
   "source": [
    "print(\"Average error results for each model:\\n\")\n",
    "\n",
    "# Loop through the transformed data and calculate MAE\n",
    "for (t1, results1), (t2, results2), (t3, results3) in grouper(3, y_yhats.items()):\n",
    "    model = t1.split('-')[1]\n",
    "    error = mean_absolute_error(results1['prediction'].argmax(axis=1),\n",
    "                                results1['ground_truth'].argmax(axis=1))/4\n",
    "    error += mean_absolute_error(results2['prediction'].argmax(axis=1),\n",
    "                                 results2['ground_truth'].argmax(axis=1))/4\n",
    "    error += mean_absolute_error(results3['prediction'].argmax(axis=1),\n",
    "                                 results3['ground_truth'].argmax(axis=1))/4\n",
    "    \n",
    "    print('Model {}: {:.2f}%'.format(model, 100*error/3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standard deviation of predictions based on actual labels:\n",
      "\n",
      "test_dl-blur: 1.00\n",
      "test_ID-blur: 0.66\n",
      "test_OOD-blur: 0.77\n",
      "test_dl-ghosting: 0.94\n",
      "test_ID-ghosting: 0.66\n",
      "test_OOD-ghosting: 0.57\n",
      "test_dl-motion: 0.97\n",
      "test_ID-motion: 0.63\n",
      "test_OOD-motion: 0.50\n",
      "test_dl-noise: 1.04\n",
      "test_ID-noise: 0.54\n",
      "test_OOD-noise: 0.51\n",
      "test_dl-resolution: 1.10\n",
      "test_ID-resolution: 0.78\n",
      "test_OOD-resolution: 0.66\n",
      "test_dl-spike: 0.89\n",
      "test_ID-spike: 1.10\n",
      "test_OOD-spike: 0.79\n"
     ]
    }
   ],
   "source": [
    "print(\"Standard deviation of predictions based on actual labels:\\n\")\n",
    "\n",
    "# Loop through the transformed data and calculate the std\n",
    "for test_name, results in y_yhats.items():\n",
    "    std = np.std(np.abs(results['prediction'].argmax(axis=1) - results['ground_truth'].argmax(axis=1)))\n",
    "    print('{}: {:.2f}'.format(test_name, std))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
